{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0165e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e35f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Absolute Percentage Error (MAPE)\n",
    "    Ignores zero targets to avoid division by zero.\n",
    "    Returns percentage.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    non_zero_mask = y_true != 0\n",
    "    if non_zero_mask.sum() == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return np.mean(\n",
    "        np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])\n",
    "    ) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36db166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_split_merged_expanded_data.csv\", parse_dates=[\"date\"])\n",
    "val   = pd.read_csv(\"val_split_merged_expanded_data.csv\", parse_dates=[\"date\"])\n",
    "test  = pd.read_csv(\"test_split_merged_expanded_data_filtered.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "print(\"Train:\", train.shape, \"Val:\", val.shape, \"Test:\", test.shape)\n",
    "print(\"Train dates:\", train[\"date\"].min(), \"→\", train[\"date\"].max())\n",
    "print(\"Val dates:\", val[\"date\"].min(), \"→\", val[\"date\"].max())\n",
    "print(\"Test dates:\", test[\"date\"].min(), \"→\", test[\"date\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf06bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    # -------------------------\n",
    "    # Calendar features\n",
    "    # -------------------------\n",
    "    df[\"Wochentag\"] = df[\"date\"].dt.day_name()\n",
    "    df[\"Month\"]     = df[\"date\"].dt.month\n",
    "    df[\"dayofyear\"] = df[\"date\"].dt.dayofyear\n",
    "\n",
    "    # Seasonality encoding (cyclical)\n",
    "    df[\"sin_season\"] = np.sin(2 * np.pi * df[\"dayofyear\"] / 365)\n",
    "    df[\"cos_season\"] = np.cos(2 * np.pi * df[\"dayofyear\"] / 365)\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"Wochentag\"].isin([\"Saturday\", \"Sunday\"]).astype(int)\n",
    "\n",
    "    # Integer/calendar flags (force 0/1 and no NaNs)\n",
    "    for col in [\"KielerWoche\", \"school_holiday\", \"public_holiday\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_fe = add_features(train)\n",
    "val_fe   = add_features(val)\n",
    "test_fe  = add_features(test)\n",
    "\n",
    "print(\"Feature engineering done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ef423",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fe[\"dataset\"] = \"train\"\n",
    "val_fe[\"dataset\"]   = \"val\"\n",
    "test_fe[\"dataset\"]  = \"test\"\n",
    "\n",
    "df_all = pd.concat([train_fe, val_fe, test_fe], ignore_index=True)\n",
    "df_all = df_all.sort_values([\"warengruppe\", \"date\"])\n",
    "\n",
    "# Lags of target (umsatz) capture short-term and weekly memory\n",
    "for lag in [1, 2, 7, 14]:\n",
    "    df_all[f\"lag_{lag}\"] = df_all.groupby(\"warengruppe\")[\"umsatz\"].shift(lag)\n",
    "\n",
    "# Rolling mean & std PER warengruppe (shift(1) avoids leakage)\n",
    "for window in [7, 14, 30]:\n",
    "    df_all[f\"roll{window}_mean\"] = (\n",
    "        df_all.groupby(\"warengruppe\")[\"umsatz\"]\n",
    "              .transform(lambda s: s.shift(1).rolling(window).mean())\n",
    "    )\n",
    "    df_all[f\"roll{window}_std\"] = (\n",
    "        df_all.groupby(\"warengruppe\")[\"umsatz\"]\n",
    "              .transform(lambda s: s.shift(1).rolling(window).std())\n",
    "    )\n",
    "\n",
    "# Sanity check: first rows per WG should have NaNs in rolling features (expected)\n",
    "print(df_all.groupby(\"warengruppe\")[[\"roll7_mean\", \"roll7_std\"]].head(3))\n",
    "\n",
    "# -------------------------\n",
    "# Missingness flags for lag/rolling features\n",
    "# (keeps information about \"empty cells\" after imputation)\n",
    "# -------------------------\n",
    "lag_roll_cols = [c for c in df_all.columns if c.startswith(\"lag_\") or c.startswith(\"roll\")]\n",
    "for c in lag_roll_cols:\n",
    "    df_all[c + \"_isna\"] = df_all[c].isna().astype(int)\n",
    "\n",
    "\n",
    "# One-hot weekday (done on ALL so columns match across splits)\n",
    "df_all = pd.get_dummies(df_all, columns=[\"Wochentag\"], drop_first=False)\n",
    "\n",
    "train_fe = df_all[df_all[\"dataset\"] == \"train\"].copy()\n",
    "val_fe   = df_all[df_all[\"dataset\"] == \"val\"].copy()\n",
    "test_fe  = df_all[df_all[\"dataset\"] == \"test\"].copy()\n",
    "\n",
    "print(\"Lag + rolling + one-hot done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_cols = [c for c in train_fe.columns if c.startswith(\"Wochentag_\")]\n",
    "\n",
    "na_flag_cols = [c for c in train_fe.columns if c.endswith(\"_isna\")]\n",
    "\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "    \"Temperatur\",\n",
    "    \"KielerWoche\",\n",
    "    \"school_holiday\",\n",
    "    \"public_holiday\",\n",
    "    \"Month\",\n",
    "    \"sin_season\",\n",
    "    \"cos_season\",\n",
    "    \"is_weekend\",\n",
    "    \"lag_1\",\n",
    "    \"lag_2\",\n",
    "    \"lag_7\",\n",
    "    \"lag_14\",\n",
    "    \"roll7_mean\",\n",
    "    \"roll7_std\",\n",
    "    \"roll14_mean\",\n",
    "    \"roll14_std\",\n",
    "    \"roll30_mean\",\n",
    "    \"roll30_std\",\n",
    "] + weekday_cols + na_flag_cols\n",
    "\n",
    "print(\"NA-flag features:\", len(na_flag_cols))\n",
    "print(\"Example NA-flag cols:\", na_flag_cols[:5])\n",
    "\n",
    "\n",
    "target_col = \"umsatz\"\n",
    "\n",
    "# Export feature dataset (train only)\n",
    "features_dataset = train_fe[feature_cols + [target_col]].copy()\n",
    "\n",
    "features_dataset.to_csv(\n",
    "    \"nn_feature_dataset_train.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Saved: nn_feature_dataset_train.csv\")\n",
    "print(\"Number of features:\", len(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4515606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim: int) -> tf.keras.Model:\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=\"relu\", input_shape=(input_dim,)),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"mse\"\n",
    "    )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e511272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_groups = sorted(train_fe[\"warengruppe\"].dropna().unique())\n",
    "\n",
    "pred_list = []\n",
    "models_by_wg = {}\n",
    "results = []\n",
    "\n",
    "for wg in product_groups:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\" Warengruppe {wg}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    train_wg = train_fe[train_fe[\"warengruppe\"] == wg].copy()\n",
    "    val_wg   = val_fe[val_fe[\"warengruppe\"] == wg].copy()\n",
    "    test_wg  = test_fe[test_fe[\"warengruppe\"] == wg].copy()\n",
    "\n",
    "    # Drop rows without target in train/val\n",
    "    train_wg = train_wg.dropna(subset=[target_col])\n",
    "    val_wg   = val_wg.dropna(subset=[target_col])\n",
    "\n",
    "    # Drop rows with missing FEATURES in train/val only\n",
    "    train_wg = train_wg.dropna(subset=feature_cols)\n",
    "    val_wg   = val_wg.dropna(subset=feature_cols)\n",
    "\n",
    "    # DO NOT drop test rows\n",
    "    print(f\"Rows: train={len(train_wg)}, val={len(val_wg)}, test={len(test_wg)}\")\n",
    "\n",
    "    if len(train_wg) < 50 or len(val_wg) < 20:\n",
    "        print(\"⚠️ Too few rows for stable NN training → skipping this WG.\")\n",
    "        continue\n",
    "\n",
    "    if len(test_wg) == 0:\n",
    "        print(\"⚠️ No test rows for this WG → skipping prediction.\")\n",
    "        continue\n",
    "\n",
    "    # ✅ Build X/y AFTER filtering\n",
    "    X_train = train_wg[feature_cols].to_numpy()\n",
    "    y_train = train_wg[target_col].to_numpy()\n",
    "\n",
    "    X_val   = val_wg[feature_cols].to_numpy()\n",
    "    y_val   = val_wg[target_col].to_numpy()\n",
    "\n",
    "    X_test  = test_wg[feature_cols].to_numpy()\n",
    "\n",
    "    # ✅ Impute\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_val   = imputer.transform(X_val)\n",
    "    X_test  = imputer.transform(X_test)\n",
    "\n",
    "    # ✅ Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val   = scaler.transform(X_val)\n",
    "    X_test  = scaler.transform(X_test)\n",
    "\n",
    "    # Train\n",
    "    model = build_model(input_dim=X_train.shape[1])\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    y_train_pred = model.predict(X_train, verbose=0).ravel()\n",
    "    y_val_pred   = model.predict(X_val, verbose=0).ravel()\n",
    "\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_val   = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mape_train = mape(y_train, y_train_pred)\n",
    "\n",
    "    mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "    mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "    mape_val = mape(y_val, y_val_pred)\n",
    "\n",
    "    print(\n",
    "        f\"NN R² (train): {r2_train:.3f} | MSE: {mse_train:.2f} | MAE: {mae_train:.2f} | MAPE: {mape_train:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"NN R² (val):   {r2_val:.3f} | MSE: {mse_val:.2f} | MAE: {mae_val:.2f} | MAPE: {mape_val:.2f}%\"\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"warengruppe\": wg,\n",
    "        \"n_val\": len(val_wg),\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_val\": r2_val,\n",
    "        \"mse_train\": mse_train,\n",
    "        \"mae_train\": mae_train,\n",
    "        \"mape_train\": mape_train,\n",
    "        \"mse_val\": mse_val,\n",
    "        \"mae_val\": mae_val,\n",
    "        \"mape_val\": mape_val\n",
    "    })\n",
    "\n",
    "    # Predict test\n",
    "    y_test_pred = model.predict(X_test, verbose=0).ravel()\n",
    "\n",
    "    pred_list.append(pd.DataFrame({\n",
    "        \"id\": test_wg[\"id\"].values,\n",
    "        \"umsatz_Prediction\": y_test_pred\n",
    "    }))\n",
    "\n",
    "    models_by_wg[wg] = (model, scaler, imputer)\n",
    "\n",
    "# ==============================\n",
    "# Combined (weighted) evaluation\n",
    "# ==============================\n",
    "results_df = pd.DataFrame(results).sort_values(\"warengruppe\")\n",
    "\n",
    "weighted_r2_train = (results_df[\"r2_train\"] * results_df[\"n_val\"]).sum() / results_df[\"n_val\"].sum()\n",
    "weighted_r2_val   = (results_df[\"r2_val\"]   * results_df[\"n_val\"]).sum() / results_df[\"n_val\"].sum()\n",
    "\n",
    "weighted_mape_val = (results_df[\"mape_val\"] * results_df[\"n_val\"]).sum() / results_df[\"n_val\"].sum()\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\" Combined (weighted) metrics \")\n",
    "print(\"==============================\")\n",
    "print(f\"Weighted R² (train): {weighted_r2_train:.3f}\")\n",
    "print(f\"Weighted R² (val):   {weighted_r2_val:.3f}\")\n",
    "print(f\"Weighted MAPE (val): {weighted_mape_val:.2f}%\")\n",
    "\n",
    "display(results_df)  # optional (notebook only)\n",
    "\n",
    "# Submission\n",
    "if len(pred_list) == 0:\n",
    "    raise ValueError(\"No predictions generated. Check test split and IDs.\")\n",
    "\n",
    "submission = pd.concat(pred_list, ignore_index=True)\n",
    "submission = submission.dropna(subset=[\"id\"]).copy()\n",
    "submission[\"id\"] = submission[\"id\"].astype(int)\n",
    "submission = submission.sort_values(\"id\")\n",
    "submission.to_csv(\"submission_neural_net.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved: submission_neural_net.csv\")\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
